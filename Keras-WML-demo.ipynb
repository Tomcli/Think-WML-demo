{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero to Singularity: Create, Tune, Deploy and Scale a Deep Neural Network in 90 Minutes\n",
    "\n",
    "This notebook is part of a masterclass held at IBM Think on 13th of February 2019 in San Fransisco\n",
    "In this exercise you will train a Keras DeepLearning model running on top of TensorFlow. \n",
    "\n",
    "Note: For sake of bringing the training runtime down we've done two things\n",
    "\n",
    "1) Used a softmax regression model over two Keras Dense layers.\n",
    "\n",
    "2) Trained only for one epoch instead of 20\n",
    "\n",
    "This leads to approx. 5% less accuracy\n",
    "\n",
    "\n",
    "Authors\n",
    "\n",
    "Romeo Kienzler - Chief Data Scientist, IBM Watson IoT\n",
    "\n",
    "Krishnamurthy Arthanarisamy - Architect, Watson Machine Learning Software Lab, Bangalore\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "Please make sure the currently installed version of Keras and Tensorflow are matching the requirememts, if not, please run the two PIP commands below in order to re-install. Please restart the kernal before proceeding, please re-check if the versions are matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print('Current:\\t', keras.__version__)\n",
    "print('Expected:\\t 2.1.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('Current:\\t', tf.__version__)\n",
    "print('Expected:\\t 1.5.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Train a MNIST digits recognition model\n",
    "We start with some global parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some learners constantly reported 502 errors in Watson Studio. \n",
    "#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n",
    "#This is a workaround to limit resource consumption\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple model\n",
    "First we'll train a simple softmax regressor and check what accuracy we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(x_test, y_test))\n",
    "        \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('\\n')\n",
    "print('Accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some cleanup from the previous run\n",
    "!rm ker_*\n",
    "!rm my_best_model.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an accuracy of approximately 90%. Now lets define a hyper-parameter grid including different activation functions and gradient descent optimizers. We’re optimizing over the grid using grid search (nested for loops) and store each model variant in a file. We then decide for the best one in order to deploy to IBM Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter grid\n",
    "\n",
    "activation_funcitons_layer_1 = ['sigmoid','tanh','relu']\n",
    "opimizers = ['rmsprop','adagrad','adadelta']\n",
    "\n",
    "#optimize over parameter grid (grid search)\n",
    "\n",
    "for activation_funciton_layer_1 in activation_funcitons_layer_1:\n",
    "    for opimizer in opimizers:\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, activation = activation_funciton_layer_1, input_shape=(784,)))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "        \n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        save_path = \"ker_func_mnist_model_2.%s.%s.%s.h5\" % (activation_funciton_layer_1,opimizer,score[1])\n",
    "        model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Let's have a look at all the models and see which hyper parameter configuration was the best one. You should see that relu and rmsprop gives you > 95% of accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -ltr ker_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create a tarball out of your favorite model, please replace the name of your favorite model H5 file with “<please-put-me-here>”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zcvf my_best_model.tgz <please-put-me-here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Save the trained model to WML Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `watson_machine_learning_client` python library to save the trained model to WML Repository, to deploy the saved model and to make predictions using the deployed model.</br>\n",
    "\n",
    "\n",
    "`watson_machine_learning_client` can be installed using the following `pip` command:\n",
    "\n",
    "`!pip install watson-machine-learning-client --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show watson-machine-learning-client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir temp_install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watson-machine-learning-client --upgrade\n",
    "!pip install -b ./temp_install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.python.org/simple/ watson-machine-learning-client==1.0.133\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload the model to Watson Machine Learning, we will need to define the credentials for the API Client. These credentials can be found at the left panel of your machine learning service page. To access your machine learning service page, please visit https://cloud.ibm.com/resources and click on the machine learning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials={\n",
    "  \"apikey\": \"\",\n",
    "  \"iam_apikey_description\": \"\",\n",
    "  \"iam_apikey_name\": \"\",\n",
    "  \"iam_role_crn\": \"\",\n",
    "  \"iam_serviceid_crn\": \"\",\n",
    "  \"instance_id\": \"\",\n",
    "  \"password\": \"\",\n",
    "  \"url\": \"\",\n",
    "  \"username\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model metadata for Watson Machine Learning to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"IBM\", \n",
    "               client.repository.ModelMetaNames.AUTHOR_EMAIL: \"ibm@ibm.com\", \n",
    "               client.repository.ModelMetaNames.NAME: \"KK3_clt_keras_mnist\",\n",
    "               client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "               client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.5\" ,\n",
    "               client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{\"name\": \"keras\", \"version\": \"2.1.3\"}]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = client.repository.store_model(model=\"my_best_model.tgz\", meta_props=model_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the model from WML Repository\n",
    "### client.repository.delete(published_model_uid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Deploy the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep your environment clean, just delete all deployments from previous runs if you have any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the deployment from previous runs\n",
    "### client.deployments.delete(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_deployment = client.deployments.create(published_model_uid, name=\"k1_keras_mnist_clt1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = created_deployment['entity']['scoring_url']\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_score_1 = x_test[23].tolist()\n",
    "print('The answer should be: ',np.argmax(y_test[23]))\n",
    "scoring_payload = {'values': [x_score_1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the scoring input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "convertedArray = []\n",
    "for i in range(0, 28):\n",
    "    temp = []\n",
    "    for j in range(0, 28):\n",
    "        temp.append(x_score_1[(i*28)+j])\n",
    "    convertedArray.append(temp)\n",
    "plt.imshow(convertedArray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with the web service endpoint on Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(scoring_endpoint, scoring_payload)\n",
    "print('And the answer is!... ',predictions['values'][0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
